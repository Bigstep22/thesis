\subsection{Performance Comparison}
I defined 11 Different functions which can be categorized into the following five groups:
\begin{itemize}[noitemsep]
\item Unfused
\item Hand fused
\item GHC List fused
\item Church fused (normally, with tail recursion, with skip constructor, and both)
\item Cochurch fused (normally, with tail recursion, with skip constructor, and both)
\end{itemize}

There are many things to take into consideration when implementing functions in haskell that are fusible:
\begin{itemize}[noitemsep]
    \item Make sure you only pass through parameters that change between recursive calls
    \item Ensure that functions are inlined properly
    \item Ensure that the fused result is tail recursive
    \item Ensure that the fused result is a single recursive function (so no helper functions such as \tt{go})
\end{itemize}

For the testing, it was ensured, that the first two of the above points were always taken into consideration, partially through analysis of the GHC\footnote{\url{https://downloads.haskell.org/ghc/latest/docs/users_guide/debugging.html\#core-representation-and-simplification}} generated Core representation.
The latter two points became part of the testing setup.
The performance tests were evaluated using tastybench\footnote{\url{https://hackage.haskell.org/package/tasty-bench}}

\subsubsection{Performance differences}
There are two main results figures, which can be seen at \autoref{fig:results1} and \autoref{fig:results2}.
However, their y axes are logarithamic, due to the nature of the input sizes provided from \tt{(1,100)} to \tt{(1,10000000)} in powers of 10.
It is more illustrative to look a linear scale, and that is easiest when zooming in on one specific input.
For the illustration, I will choose \tt{(1,10000)} as input, as it is relatively representative. There specific are variations when changing scale, but those will be discussed in \autoref{sec:scale_var}.

In \autoref{fig:res_1000} you can see how implementing fusion can bring quite a large speedup to a function pipeline.
With the following things of note:
\begin{itemize}[noitemsep]
    \item Tail recursive church, stream church, and stream cochurch implementations were the fastest, and as fast as each other. A speedup of 25x over the unfused pipeline for this input.
    \item Stream fusion does not offer a speedup for Church encodings
    \item Adding tail recursion speeds up the encoding in all cases, except:
    \item Church-encoded non-stream pipelines are not faster, this is due to a recursive natural transformation for filter.
\end{itemize}

\begin{figure}[h] % TODO: Fix this figure
    \includesvg[width=\textwidth]{figures/Execution time for input (1, 10000).svg}
    \caption{Temp1}
    \label{fig:res_1000}
\end{figure}

\subsubsection{Scale Variations}\label{sec:scale_var}
As the scale increases (see \autoref{fig:all_exec} and \autoref{fig:all_exec_log} for all graphs):
\begin{itemize}[noitemsep]
    \item The factor speedup between the unfused and three fully fused pipelines increases, most notably between \tt{(1, 10000)} and \tt{(1,100000)}, going from 25.4x to 31.5x.
    This might have to do with the generated datastructures not fitting in cache anymore and needing to be pushed to ram.
    \item All the non tail-recursive encoding implementations get slower. This is likely due to extra time spend allocating to and retrieving from memory.
    \item Most importantly for all fully fused pipelines: The speedup that fusion offers only seems to increase as the size of the calculation grows.
\end{itemize}

\begin{figure}[h]
    \includesvg[width=\textwidth]{figures/Execution time for all pipelines.svg}
    \caption{Temp1}
    \label{fig:results1}
\end{figure}
\begin{figure}[h]
    \includesvg[width=\textwidth]{figures/Execution time for all pipelines - stacked.svg}
    \caption{Temp1}
    \label{fig:results2}
\end{figure}
